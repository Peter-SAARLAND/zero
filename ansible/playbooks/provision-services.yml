- hosts: manager[0]
  become: True
  gather_facts: True
  remote_user: "{{ deploy_user_name }}"
  tasks:
    - name: Create Swarm Overlay networks
      docker_network:
        name: "{{ item }}"
        driver: overlay
        attachable: True
      with_items: "{{ swarm_networks }}"

    - name: Generate admin password hash
      shell: "docker run --rm httpd:2.4-alpine htpasswd -nbB admin '{{ admin_password }}' | cut -d ':' -f 2"
      register: admin_password_hash
      tags:
        - portainer
        - traefik
        - consul
        - prometheus
      changed_when: false

    - debug:
        var: admin_password_hash
        verbosity: 1
      tags:
        - traefik
        - portainer
        - consul
        - prometheus

    - name: Create CNAME for Traefik
      cloudflare_dns:
        zone: "{{ base_domain }}"
        record: "proxy"
        type: CNAME
        value: "swarm.{{base_domain}}"
        account_email: "{{ cf_api_email }}"
        account_api_token: "{{ cf_api_key }}"
        solo: True
      register: record

    - name: Provision Traefik
      docker_swarm_service:
        name: traefik
        state: present
        mode: replicated
        replicas: 1
        image: traefik:v1.7.19-alpine
        resolve_image: True
        command: 
          traefik --docker --docker.swarmmode --docker.watch --docker.exposedbydefault=false --insecureSkipVerify --constraints=tag==proxy --entrypoints="Name:http Address::80" --entrypoints="Name:https Address::443 TLS" --metrics --metrics.prometheus --acme --acme.email={{ letsencrypt_mail }} --acme.storage="/etc/traefik/acme.json" --acme.entryPoint=https --acme.dnsChallenge.provider=cloudflare --acme.dnsChallenge.resolvers="1.1.1.1:53" --acme.onhostrule=true --acme.acmelogging=true --logLevel=DEBUG --accessLog --api --debug --docker.domain={{ base_domain }} --logLevel=DEBUG --ping --rest --retry
        env:
          CF_API_EMAIL: "{{ cf_api_email }}"
          CF_API_KEY: "{{ cf_api_key }}"
        networks:
          - name: "proxy"
            aliases:
              - "traefik"
            options:
              driver: "overlay"
        publish:
          - mode: ingress
            protocol: tcp
            published_port: 443
            target_port: 443
          - mode: ingress
            protocol: tcp
            published_port: 80
            target_port: 80
        mounts:
          - source: traefik-config-cio
            target: /etc/traefik
            type: volume
            readonly: no
            driver_config: 
              name: "cio"
          - source: /var/run/docker.sock
            target: /var/run/docker.sock
            type: bind
            readonly: True
        update_config:
          parallelism: 2
          delay: 60s
          monitor: 30s
          order: start-first
          failure_action: rollback
        restart_config:
          condition: on-failure
        placement:
          constraints: 
            - node.role == manager
        labels:
          traefik.enable: "true"
          traefik.tags: "proxy"
          traefik.docker.network: "proxy"
          traefik.port: "8080"
          traefik.frontend.rule: "Host:proxy.{{ base_domain }}"
          traefik.backend: "traefik"
          traefik.protocol: "http"
          traefik.frontend.headers.SSLRedirect: "true"
          traefik.frontend.entryPoints: "http,https"
          traefik.backend.loadbalancer.stickiness: "true"
          traefik.frontend.auth.basic.users: "{{admin_user}}:{{admin_password_hash.stdout}}"
      register: traefik_provision
      tags:
        - traefik

    - debug:
        var: traefik_provision
        verbosity: 1
      tags:
        - traefik
      when: traefik_provision.changed

    - name: "Wait for proxy.{{ base_domain }} to become ready"
      uri:
        url: "https://proxy.{{ base_domain }}"
        method: GET
        status_code: 200
        user: "{{admin_user}}"
        password: "{{admin_password}}"
      register: _result
      until: _result.status == 200
      retries: 30 # retry X times  
      delay: 5 # pause for X sec b/w each call 
      tags:
        - traefik
    
    - name: Provision Portainer Agent
      docker_swarm_service:
        name: portainer-agent
        state: present
        mode: global
        image: portainer/agent:1.5.1
        resolve_image: True
        env:
          AGENT_CLUSTER_ADDR: "tasks.portainer-agent"
          CAP_HOST_MANAGEMENT: "1"
        networks:
          - name: "portainer"
            aliases:
              - "portainer"
            options:
              driver: "overlay"
        mounts:
          - source: /var/lib/docker/volumes
            target: /var/lib/docker/volumes
            type: bind
            readonly: no
          - source: /var/run/docker.sock
            target: /var/run/docker.sock
            type: bind
            readonly: no
        restart_config:
          condition: on-failure
        update_config:
          parallelism: 2
          delay: 60s
          monitor: 30s
          order: start-first
          failure_action: rollback
        placement:
          constraints: 
            - node.platform.os == linux
      register: portainer_agent_provision

    - name: Pause until Service is up
      pause: 
        prompt: "Make sure portainer-agent is up and running"
        seconds: 30
      when: portainer_agent_provision.changed

    - name: Provision Portainer
      docker_swarm_service:
        command: "/portainer -H tcp://tasks.portainer-agent:9001 --tlsskipverify --logo https://www.micro-biolytics.com/wp-content/uploads/2019/03/cropped-mBio_logo.png --no-analytics --admin-password {{ admin_password_hash.stdout }}"
        name: portainer
        state: present
        mode: replicated
        replicas: 1
        image: portainer/portainer:1.23.0
        resolve_image: True
        env:
          AGENT_CLUSTER_ADDR: "tasks.portainer-agent"
          CAP_HOST_MANAGEMENT: "1"
        networks:
          - name: "proxy"
            aliases:
              - "traefik"
            options:
              driver: "overlay"
          - name: "portainer"
            aliases:
              - "portainer"
            options:
              driver: "overlay"
        mounts:
          - source: portainer-data-cio
            target: /data
            type: volume
            readonly: no
            driver_config: 
              name: cio
              options:
                snapshot: yes
                interval: 60
                snapshotMax: 12
        restart_config:
          condition: on-failure
        update_config:
          parallelism: 2
          delay: 60s
          monitor: 30s
          order: start-first
          failure_action: rollback
        placement:
          constraints: 
            - node.role == manager
        labels:
          traefik.enable: "true"
          traefik.tags: "proxy"
          traefik.docker.network: "proxy"
          traefik.port: "9000"
          traefik.frontend.rule: "Host:swarm.{{ base_domain }}"
          traefik.backend: "portainer-swarm"
          traefik.protocol: "http"
          traefik.frontend.headers.SSLRedirect: "true"
          traefik.frontend.entryPoints: "http,https"
          traefik.backend.loadbalancer.stickiness: "true"
      register: portainer_provision
      tags:
        - portainer

    - debug:
        var: portainer_provision
        verbosity: 1
      tags:
        - portainer
      when: portainer_provision.changed

    - name: "Wait for swarm.{{ base_domain }} to become ready"
      uri:
        url: "https://swarm.{{ base_domain }}"
        method: GET
        status_code: 200
      register: _result
      until: _result.status == 200
      retries: 30 # retry X times  
      delay: 5 # pause for X sec b/w each call
      tags:
        - portainer

    - name: Provision cadvisor
      docker_swarm_service:
        name: cadvisor
        state: present
        mode: global
        image: gcr.io/google-containers/cadvisor:v0.34.0
        resolve_image: True
        command: /usr/bin/cadvisor -logtostderr -docker_only
        networks:
          - name: "monitoring"
            aliases:
              - "monitoring"
            options:
              driver: "overlay"
        mounts:
          - source: /
            target: /rootfs
            type: bind
            readonly: True
          - source: /var/run/docker.sock
            target: /var/run/docker.sock
            type: bind
            readonly: True
          - source: /var/run
            target: /var/run
            type: bind
            readonly: True
          - source: /sys
            target: /sys
            type: bind
            readonly: True
          - source: /var/lib/docker
            target: /var/lib/docker
            type: bind
            readonly: True
          - source: /dev/disk
            target: /dev/disk
            type: bind
            readonly: True
        restart_config:
          condition: on-failure
        labels:
          prometheus.enable: "true"
          prometheus.port: "8080"
          prometheus.path: "/metrics"
        limits:
          memory: 128M
        reservations:
          memory: 64M
        update_config:
          parallelism: 2
          delay: 60s
          monitor: 30s
          order: start-first
          failure_action: rollback
        placement:
          constraints: 
            - node.platform.os == linux
      register: provision_cadvisor

    - name: Pause until Service is up
      pause: 
        prompt: "Make sure cadvisor is up and running"
        seconds: 30
      when: provision_cadvisor.changed

    - name: Provision cio-exporter
      docker_swarm_service:
        name: cio-exporter
        state: present
        mode: replicated
        replicas: 1
        image: storidge/cio-prom:latest
        resolve_image: True
        networks:
          - name: "monitoring"
            aliases:
              - "monitoring"
            options:
              driver: "overlay"
        restart_config:
          condition: on-failure
        labels:
          prometheus.enable: "true"
          prometheus.port: "16995"
          prometheus.path: "/metrics"
        limits:
          memory: 128M
        reservations:
          memory: 64M
        update_config:
          parallelism: 2
          delay: 60s
          monitor: 30s
          order: start-first
          failure_action: rollback
        placement:
          constraints: 
            - node.role == manager
      register: cio_exporter_provision

    - name: Pause until Service is up
      pause: 
        prompt: "Make sure cio-exporter is up and running"
        seconds: 30
      when: cio_exporter_provision.changed

    - name: Provision dockerd-exporter
      docker_swarm_service:
        name: dockerd-exporter
        state: present
        mode: global
        image: stefanprodan/dockerd-exporter:latest
        resolve_image: True
        env:
          IN: "172.18.0.1:9323"
          OUT: "9323"
        networks:
          - name: "monitoring"
            aliases:
              - "monitoring"
            options:
              driver: "overlay"
        restart_config:
          condition: on-failure
        labels:
          prometheus.enable: "true"
          prometheus.port: "8080"
          prometheus.path: "/metrics"
        limits:
          memory: 128M
        reservations:
          memory: 64M
        update_config:
          parallelism: 2
          delay: 60s
          monitor: 30s
          order: start-first
          failure_action: rollback
        placement:
          constraints: 
            - node.platform.os == linux
      register: provision_dockerd_exporter
      tags:
        - dockerd-exporter

    - debug:
        var: provision_dockerd_exporter
        verbosity: 1
      tags:
        - dockerd-exporter
      when: provision_dockerd_exporter.changed

    - name: Pause until Service is up
      pause: 
        prompt: "Make sure dockerd-exporter is up and running"
        seconds: 30
      when: provision_dockerd_exporter.changed
      tags:
        - dockerd-exporter

    - name: Log into docker registry
      docker_login:
        registry: "{{ registry_url }}"
        username: "{{ registry_user }}"
        password: "{{ registry_password }}"
        reauthorize: no
      tags:
        - node-exporter

    - name: Provision node-exporter
      docker_swarm_service:
        name: node-exporter
        state: present
        mode: global
        image: registry.gitlab.com/mbio/mbiosphere/infrastructure/node-exporter:latest
        resolve_image: True
        networks:
          - name: "monitoring"
            aliases:
              - "monitoring"
            options:
              driver: "overlay"
        env:
          - NODE_ID={{ "{{.Node.ID}}" }}
        mounts:
          - source: /
            target: /rootfs
            type: bind
            readonly: True
          - source: /var/run/docker.sock
            target: /var/run/docker.sock
            type: bind
            readonly: True
          - source: /proc
            target: /host/proc
            type: bind
            readonly: True
          - source: /sys
            target: /host/sys
            type: bind
            readonly: True
          - source: /etc/hostname
            target: /etc/nodename
            type: bind
            readonly: True
        restart_config:
          condition: on-failure
        limits:
          memory: 128M
        reservations:
          memory: 64M
        update_config:
          parallelism: 2
          delay: 60s
          monitor: 30s
          order: start-first
          failure_action: rollback
        placement:
          constraints: 
            - node.platform.os == linux
      tags:
        - node-exporter
      register: node_exporter_provision

    - debug:
        var: node_exporter_provision
        verbosity: 1
      tags:
        - node-exporter
      when: node_exporter_provision.changed

    - name: Pause until Service is up
      pause: 
        prompt: "Make sure node-exporter is up and running"
        seconds: 30
      tags:
        - node-exporter
      when: node_exporter_provision.changed

    - name: Provision promtail
      docker_swarm_service:
        name: promtail
        state: present
        mode: global
        image: grafana/promtail:latest
        resolve_image: True
        command: /usr/bin/promtail -config.file=/etc/promtail/docker-config.yaml
        networks:
          - name: "monitoring"
            aliases:
              - "monitoring"
            options:
              driver: "overlay"
        mounts:
          - source: /var/log
            target: /var/log
            type: bind
            readonly: True
        restart_config:
          condition: on-failure
        limits:
          memory: 128M
        reservations:
          memory: 64M
        update_config:
          parallelism: 2
          delay: 60s
          monitor: 30s
          order: start-first
          failure_action: rollback
        placement:
          constraints: 
            - node.platform.os == linux
      register: provision_promtail
      tags:
        - promtail
        - monitoring

    - debug:
        var: provision_promtail
        verbosity: 1
      tags:
        - promtail
      when: provision_promtail.changed

    - name: Pause until Service is up
      pause: 
        prompt: "Make sure promtail is up and running"
        seconds: 30
      when: provision_promtail.changed
      tags:
        - promtail

    - name: Provision Docker garbage-collector
      docker_swarm_service:
        name: garbage-collector
        state: present
        mode: global
        image: clockworksoul/docker-gc-cron:latest
        resolve_image: True
        mounts:
          - source: /var/run/docker.sock
            target: /var/run/docker.sock
            type: bind
            readonly: no
        restart_config:
          condition: on-failure
        env:
          - CRON=* 15 * * *
          # By default, docker will not remove an image if it is tagged in multiple repositories. If
          # you have a server running docker where this is the case, for example in CI environments
          # where dockers are being built, re-tagged, and pushed, you can set this flag to 1 to override.
          - FORCE_IMAGE_REMOVAL=1
          # By default, if an error is encountered when cleaning up a container, Docker will report the
          # error back and leave it on disk. This can sometimes lead to containers accumulating. If
          # you run into this issue, you can force the removal of the container by setting this flag.
          - FORCE_CONTAINER_REMOVAL=1
          # By default, docker-gc will not remove a container if it exited less than 1 hour ago.
          # Set the GRACE_PERIOD_SECONDS variable to override this default.
          - GRACE_PERIOD_SECONDS=3600
          # By default, docker-gc will proceed with deletion of containers and images. To test your
          # settings set the DRY_RUN variable to override this default
          - DRY_RUN=0
          # By default, this process will leave any dangling volumes untouched. To instruct the
          # process to automatically clean up any dangling volumes, simply set this value to 1.
          - CLEAN_UP_VOLUMES=0
          # If you don't like all your log output and cron times being in UTC, you can set the
          # TZ variable to override the default.
          #- TZ=America/Chicago
          - MINIMUM_IMAGES_TO_SAVE=2
          - CLEAN_UP_VOLUMES=1
        limits:
          memory: 128M
        reservations:
          memory: 64M
        update_config:
          parallelism: 2
          delay: 60s
          monitor: 30s
          order: start-first
          failure_action: rollback
        placement:
          constraints: 
            - node.platform.os == linux
      tags:
        - garbage-collector
      register: garbage_collector_provision

    - debug:
        var: garbage_collector_provision
        verbosity: 1
      tags:
        - garbage-collector
      when: garbage_collector_provision.changed

    - name: Pause until Service is up
      pause: 
        prompt: "Make sure gargabe-collector is up and running"
        seconds: 30
      tags:
        - garbage-collector
      when: garbage_collector_provision.changed

    - name: Provision Consul Leader
      docker_swarm_service:
        command: >
          /usr/local/bin/docker-entrypoint.sh agent -server -client=0.0.0.0 -bootstrap -ui
        name: consul-leader
        state: present
        mode: replicated
        replicas: 1
        image: consul:1.6.2
        resolve_image: True
        env:
          CONSUL_BIND_INTERFACE: "eth0"
          CONSUL_LOCAL_CONFIG: '{"leave_on_terminate": true}'
        networks:
          - consul
          - proxy
        mounts:
          - source: consul-data-leader-cio
            target: /consul/data
            type: volume
            readonly: no
            driver_config: 
              name: cio
        restart_config:
          condition: on-failure
        placement:
          constraints:
            - node.role == manager
        labels:
          traefik.enable: "true"
          traefik.tags: "proxy"
          traefik.docker.network: "proxy"
          traefik.port: "8500"
          traefik.frontend.rule: "Host:consul.{{ base_domain }}"
          traefik.backend: "consul"
          traefik.protocol: "http"
          traefik.frontend.headers.SSLRedirect: "true"
          traefik.frontend.entryPoints: "http,https"
          traefik.backend.loadbalancer.stickiness: "true"
          traefik.frontend.auth.basic.users: "{{admin_user}}:{{admin_password_hash.stdout}}"
      register: consul_leader_provision
      tags:
        - consul-leader

    - debug:
        var: consul_leader_provision
        verbosity: 1
      tags:
        - consul-leader
      when: consul_leader_provision.changed

    - name: "Wait for consul.{{ base_domain }} to become ready"
      uri:
        url: "https://consul.{{ base_domain }}"
        method: GET
        status_code: 200
        user: "{{admin_user}}"
        password: "{{admin_password}}"
      register: _result
      until: _result.status == 200
      retries: 30 # retry X times  
      delay: 5 # pause for X sec b/w each call 
      tags:
        - consul-leader

    - name: Log into docker registry
      docker_login:
        registry: "{{ registry_url }}"
        username: "{{ registry_user }}"
        password: "{{ registry_password }}"
        reauthorize: no
      tags:
        - prometheus
        - monitoring

    - name: Provision Prometheus
      docker_swarm_service:
        command: 
          - "/bin/prometheus"
          - "--config.file=/etc/prometheus/prometheus.yml"
          #- "--web.console.libraries=/etc/prometheus/console_libraries"
          #- "--web.console.templates=/etc/prometheus/consoles"
          - "--storage.tsdb.path=/prometheus"
          - "--storage.tsdb.retention.time=48h"
          - "--web.enable-admin-api"
        name: prometheus
        resolve_image: True
        state: present
        mode: replicated
        replicas: 1
        image: registry.gitlab.com/mbio/mbiosphere/infrastructure/prometheus:latest
        networks:
          - name: "proxy"
            aliases:
              - "traefik"
            options:
              driver: "overlay"
          - name: "monitoring"
            aliases:
              - "monitoring"
            options:
              driver: "overlay"
        mounts:
          - source: prometheus-data
            target: /prometheus
            type: volume
            readonly: no
        restart_config:
          condition: on-failure
        update_config:
          parallelism: 2
          delay: 60s
          monitor: 30s
          order: stop-first
          failure_action: rollback
        placement:
          constraints: 
            - node.hostname == satellite-1
        limits:
          memory: 2048M
        reservations:
          memory: 1024M
        labels:
          traefik.enable: "true"
          traefik.tags: "proxy"
          traefik.docker.network: "proxy"
          traefik.port: "9090"
          traefik.frontend.rule: "Host:prometheus.{{ base_domain }}"
          traefik.backend: "prometheus"
          traefik.protocol: "http"
          traefik.frontend.headers.SSLRedirect: "true"
          traefik.frontend.entryPoints: "http,https"
          traefik.backend.loadbalancer.stickiness: "true"
          traefik.frontend.auth.basic.users: "{{admin_user}}:{{admin_password_hash.stdout}}"
          prometheus.enable: "true"
          prometheus.port: "9090"
          prometheus.path: "/metrics"
        healthcheck:
          test: "/bin/wget -q -Y off http://localhost:9090/status -O /dev/null > /dev/null 2>&1"
          interval: 25s
          timeout: 3s
          start_period: 60s
      register: prometheus_provision
      tags:
        - prometheus
        - monitoring

    - name: Create CNAME for Prometheus
      cloudflare_dns:
        zone: "{{ base_domain }}"
        record: "prometheus"
        type: CNAME
        value: "swarm.{{ base_domain }}"
        account_email: "{{ cf_api_email }}"
        account_api_token: "{{ cf_api_key }}"
        solo: True
      register: record
      tags:
        - prometheus
        - monitoring

    - debug:
        var: prometheus_provision
        verbosity: 1
      tags:
        - prometheus
        - monitoring
      when: prometheus_provision.changed

    - name: "Wait for prometheus.{{ base_domain }} to become ready"
      uri:
        url: "https://prometheus.{{ base_domain }}"
        method: GET
        status_code: 200
        user: "{{admin_user}}"
        password: "{{admin_password}}"
      register: _result
      until: _result.status == 200
      retries: 30 # retry X times  
      delay: 5 # pause for X sec b/w each call
      tags:
        - prometheus
        - monitoring

    - name: Provision Loki
      docker_swarm_service:
        name: loki
        resolve_image: True
        state: present
        mode: replicated
        replicas: 1
        image: grafana/loki:latest
        networks:
          - name: "monitoring"
            aliases:
              - "monitoring"
            options:
              driver: "overlay"
        mounts:
          - source: loki-data
            target: /data
            type: volume
            readonly: no
        restart_config:
          condition: on-failure
        update_config:
          parallelism: 2
          delay: 60s
          monitor: 30s
          order: stop-first
          failure_action: rollback
        placement:
          constraints: 
            - node.hostname == satellite-1
        limits:
          memory: 2048M
        reservations:
          memory: 1024M
      register: loki_provision
      tags:
        - loki
        - monitoring

    - debug:
        var: loki_provision
        verbosity: 1
      tags:
        - loki
        - monitoring
      when: loki_provision.changed

    - name: Provision Grafana
      docker_swarm_service:
        name: grafana
        state: present
        mode: replicated
        resolve_image: True
        replicas: 1
        image: registry.gitlab.com/mbio/mbiosphere/infrastructure/grafana:latest
        env:
          GF_SECURITY_ADMIN_USER: "{{ admin_user }}"
          GF_SECURITY_ADMIN_PASSWORD: "{{ admin_password }}"
          GF_USERS_ALLOW_SIGN_UP: "false"
        networks:
          - name: "proxy"
            aliases:
              - "traefik"
            options:
              driver: "overlay"
          - name: "monitoring"
            aliases:
              - "monitoring"
            options:
              driver: "overlay"
        mounts:
          - source: grafana-data
            target: /var/lib/grafana
            type: volume
            readonly: no
        restart_config:
          condition: on-failure
        update_config:
          parallelism: 2
          delay: 60s
          monitor: 30s
          order: start-first
          failure_action: rollback
        placement:
          constraints: 
            - node.hostname == satellite-1
        limits:
          memory: 1024M
        reservations:
          memory: 128M
        labels:
          traefik.enable: "true"
          traefik.tags: "proxy"
          traefik.docker.network: "proxy"
          traefik.port: "3000"
          traefik.frontend.rule: "Host:grafana.{{ base_domain }}"
          traefik.backend: "grafana"
          traefik.protocol: "http"
          traefik.frontend.headers.SSLRedirect: "true"
          traefik.frontend.entryPoints: "http,https"
          traefik.backend.loadbalancer.stickiness: "true"
          prometheus.enable: "true"
          prometheus.port: "3000"
          prometheus.path: "/metrics"
        healthcheck:
          test: "wget -q -Y off -O /dev/null http://localhost:3000/login > /dev/null 2>&1"
          interval: 25s
          timeout: 3s
          start_period: 120s
      register: grafana_provision
      tags:
        - grafana
        - monitoring

    - name: Create CNAME for Grafana
      cloudflare_dns:
        zone: "{{ base_domain }}"
        record: "grafana"
        type: CNAME
        value: "swarm.{{ base_domain }}"
        account_email: "{{ cf_api_email }}"
        account_api_token: "{{ cf_api_key }}"
        solo: True
      register: record
      tags:
        - grafana
        - monitoring

    - debug:
        var: grafana_provision
        verbosity: 1
      tags:
        - grafana
        - monitoring
      when: grafana_provision.changed

    - name: "Wait for grafana.{{ base_domain }} to become ready"
      uri:
        url: "https://grafana.{{ base_domain }}"
        method: GET
        status_code: 200
      register: _result
      until: _result.status == 200
      retries: 30 # retry X times  
      delay: 5 # pause for X sec b/w each call
      tags:
        - grafana
        - monitoring

    - name: Provision minio S3
      docker_swarm_service:
        command: 
          - "/usr/bin/docker-entrypoint.sh"
          - "minio"
          - "server"
          - "/data"
        name: minio
        resolve_image: True
        state: present
        mode: replicated
        replicas: 1
        image: minio/minio:latest
        env:
          MINIO_ACCESS_KEY: "{{admin_user}}"
          MINIO_SECRET_KEY: "{{admin_password}}"
        networks:
          - name: "proxy"
            aliases:
              - "traefik"
            options:
              driver: "overlay"
          - name: "monitoring"
            aliases:
              - "monitoring"
            options:
              driver: "overlay"
          - name: "s3"
            aliases:
              - "s3"
            options:
              driver: "overlay"
        mounts:
          - source: minio-data
            target: /data
            type: volume
            readonly: no
        restart_config:
          condition: on-failure
        update_config:
          parallelism: 2
          delay: 60s
          monitor: 30s
          order: stop-first
          failure_action: rollback
        placement:
          constraints: 
            - node.hostname == satellite-2
        limits:
          memory: 1024M
        reservations:
          memory: 512M
        labels:
          traefik.enable: "true"
          traefik.tags: "proxy"
          traefik.docker.network: "proxy"
          traefik.port: "9000"
          traefik.frontend.rule: "Host:s3.{{ base_domain }}"
          traefik.backend: "s3"
          traefik.protocol: "http"
          traefik.frontend.headers.SSLRedirect: "true"
          traefik.frontend.entryPoints: "http,https"
          traefik.backend.loadbalancer.stickiness: "true"
          #traefik.frontend.auth.basic.users: "{{admin_user}}:{{admin_password_hash.stdout}}"
        healthcheck:
          test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
          interval: 30s
          timeout: 20s
          retries: 3
      register: minio_provision
      tags:
        - minio

    - name: Create CNAME for Minio
      cloudflare_dns:
        zone: "{{ base_domain }}"
        record: "s3"
        type: CNAME
        value: "swarm1.{{ base_domain }}"
        account_email: "{{ cf_api_email }}"
        account_api_token: "{{ cf_api_key }}"
        solo: True
      register: record
      tags:
        - minio

    - debug:
        var: minio_provision
        verbosity: 1
      tags:
        - minio
      when: minio_provision.changed

    - name: "Wait for s3.{{ base_domain }} to become ready"
      uri:
        url: "https://s3.{{ base_domain }}"
        method: GET
        status_code: 200
        user: "{{admin_user}}"
        password: "{{admin_password}}"
        headers:
            Authorization: "{{admin_password}}"
      register: _result
      until: _result.status == 200
      retries: 30 # retry X times  
      delay: 5 # pause for X sec b/w each call
      tags:
        - minio
      when: minio_provision.changed
    
    # - name: Provision Consul Replicas
    #   docker_swarm_service:
    #     command: >
    #      /usr/local/bin/docker-entrypoint.sh agent -server -client=0.0.0.0 -retry-join="consul-leader"
    #     name: consul-replica
    #     state: present
    #     mode: replicated
    #     replicas: 3
    #     image: consul:1.6.2
    #     env:
    #       CONSUL_BIND_INTERFACE: "eth0"
    #       CONSUL_LOCAL_CONFIG: '{"leave_on_terminate": true}'
    #     networks:
    #       - consul
    #       - proxy
    #     mounts:
    #       - source: '{{ "{{.Service.Name}}-{{.Task.Slot}}" }}'
    #         target: /consul/data
    #         type: volume
    #         readonly: no
    #         driver_config: 
    #           name: cio
    #           # options:
    #           #   volume: '{{ "{{.Service.Name}}-{{.Task.Slot}}" }}'
    #     restart_config:
    #       condition: on-failure
    #     update_config:
    #       parallelism: 1
    #       delay: 120s
    #       monitor: 120s
    #       order: start-first
    #       failure_action: rollback
    #     placement:
    #       constraints: 
    #         - node.role == manager
    #       preferences:
    #         - spread: node.id
    #   register: provision_consul_replica

    # - name: Pause until Service is up
    #   pause: 
    #     prompt: "Make sure consul-replica is up and running"
    #     seconds: 30
    #   when: provision_consul_replica.changed

    
