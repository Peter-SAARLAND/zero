- hosts: manager[0]
  become: true
  gather_facts: false
  remote_user: "{{ deploy_user_name }}"
  tasks:
    - name: Create Swarm Overlay networks
      docker_network:
        name: "{{ item }}"
        driver: overlay
        attachable: true
      with_items: "{{ swarm_networks }}"

    - name: Provision Traefik
      docker_swarm_service:
        name: traefik
        state: present
        mode: replicated
        replicas: 1
        image: traefik:v1.7.19-alpine
        command: 
          traefik --docker --docker.swarmmode --docker.watch --docker.exposedbydefault=false --insecureSkipVerify --constraints=tag==proxy --entrypoints="Name:http Address::80" --entrypoints="Name:https Address::443 TLS" --metrics --metrics.prometheus --acme --acme.email={{ letsencrypt_mail }} --acme.storage="/etc/traefik/acme.json" --acme.entryPoint=https --acme.dnsChallenge.provider=cloudflare --acme.dnsChallenge.resolvers="1.1.1.1:53" --acme.onhostrule=true --acme.acmelogging=true --logLevel=DEBUG --accessLog --api --debug --docker.domain={{ base_domain }} --logLevel=DEBUG --ping --rest --retry
        env:
          CF_API_EMAIL: "{{ cf_api_email }}"
          CF_API_KEY: "{{ cf_api_key }}"
        networks:
          - name: "proxy"
            aliases:
              - "traefik"
            options:
              driver: "overlay"
        publish:
          - mode: ingress
            protocol: tcp
            published_port: 443
            target_port: 443
          - mode: ingress
            protocol: tcp
            published_port: 80
            target_port: 80
        mounts:
          - source: traefik-config-cio
            target: /etc/traefik
            type: volume
            readonly: yes
            driver_config: 
              name: "cio"
          - source: /var/run/docker.sock
            target: /var/run/docker.sock
            type: bind
            readonly: yes
        update_config:
          parallelism: 2
          delay: 60s
          monitor: 30s
          order: start-first
          failure_action: rollback
        restart_config:
          condition: on-failure
        placement:
          constraints: 
            - node.role == manager
      register: traefik_provision
      tags:
        - traefik

    - name: Pause until Service is up
      pause: 
        prompt: "Make sure traefik is up and running"
        seconds: 30
      when: traefik_provision.changed

    - name: Get traefik Service Info
      docker_swarm_info:
        services: yes
        services_filters:
          name: traefik
        verbose_output: yes
      register: traefik_service
      tags:
        - traefik

    - debug:
        var: traefik_service
    - debug:
        var: traefik_provision
    
    - name: Provision Portainer Agent
      docker_swarm_service:
        name: portainer-agent
        state: present
        mode: global
        image: portainer/agent:1.5.1
        env:
          AGENT_CLUSTER_ADDR: "tasks.portainer-agent"
          CAP_HOST_MANAGEMENT: "1"
        networks:
          - name: "portainer"
            aliases:
              - "portainer"
            options:
              driver: "overlay"
        mounts:
          - source: /var/lib/docker/volumes
            target: /var/lib/docker/volumes
            type: bind
            readonly: no
          - source: /var/run/docker.sock
            target: /var/run/docker.sock
            type: bind
            readonly: no
        restart_config:
          condition: on-failure
        update_config:
          parallelism: 2
          delay: 60s
          monitor: 30s
          order: start-first
          failure_action: rollback
        placement:
          constraints: 
            - node.platform.os == linux
      register: portainer_agent_provision

    - name: Pause until Service is up
      pause: 
        prompt: "Make sure portainer-agent is up and running"
        seconds: 30
      when: portainer_agent_provision.changed

    - name: Provision cadvisor
      docker_swarm_service:
        name: cadvisor
        state: present
        mode: global
        image: gcr.io/google-containers/cadvisor:v0.34.0
        command: /usr/bin/cadvisor -logtostderr -docker_only
        networks:
          - name: "monitoring"
            aliases:
              - "monitoring"
            options:
              driver: "overlay"
        mounts:
          - source: /
            target: /rootfs
            type: bind
            readonly: yes
          - source: /var/run/docker.sock
            target: /var/run/docker.sock
            type: bind
            readonly: yes
          - source: /var/run
            target: /var/run
            type: bind
            readonly: yes
          - source: /sys
            target: /sys
            type: bind
            readonly: yes
          - source: /var/lib/docker
            target: /var/lib/docker
            type: bind
            readonly: yes
          - source: /dev/disk
            target: /dev/disk
            type: bind
            readonly: yes
        restart_config:
          condition: on-failure
        labels:
          prometheus.enable: "true"
          prometheus.port: "8080"
          prometheus.path: "/metrics"
        limits:
          memory: 128M
        reservations:
          memory: 64M
        update_config:
          parallelism: 2
          delay: 60s
          monitor: 30s
          order: start-first
          failure_action: rollback
        placement:
          constraints: 
            - node.platform.os == linux
      register: provision_cadvisor

    - name: Pause until Service is up
      pause: 
        prompt: "Make sure cadvisor is up and running"
        seconds: 30
      when: provision_cadvisor.changed

    - name: Log into docker registry and force re-authorization
      docker_login:
        registry: "{{ registry_url }}"
        username: "{{ registry_user }}"
        password: "{{ registry_password }}"
        reauthorize: yes
      tags:
        - node-exporter

    - name: Provision node-exporter
      docker_swarm_service:
        name: node-exporter
        state: present
        mode: global
        image: registry.gitlab.com/mbio/mbiosphere/infrastructure/node-exporter:latest
        networks:
          - name: "monitoring"
            aliases:
              - "monitoring"
            options:
              driver: "overlay"
        env:
          - NODE_ID={{ "{{.Node.ID}}" }}
        mounts:
          - source: /
            target: /rootfs
            type: bind
            readonly: yes
          - source: /var/run/docker.sock
            target: /var/run/docker.sock
            type: bind
            readonly: yes
          - source: /proc
            target: /host/proc
            type: bind
            readonly: yes
          - source: /sys
            target: /host/sys
            type: bind
            readonly: yes
          - source: /etc/hostname
            target: /etc/nodename
            type: bind
            readonly: yes
        restart_config:
          condition: on-failure
        limits:
          memory: 128M
        reservations:
          memory: 64M
        update_config:
          parallelism: 2
          delay: 60s
          monitor: 30s
          order: start-first
          failure_action: rollback
        placement:
          constraints: 
            - node.platform.os == linux
      tags:
        - node-exporter
      register: provision_node_exporter

    - name: Pause until Service is up
      pause: 
        prompt: "Make sure node-exporter is up and running"
        seconds: 30
      tags:
        - node-exporter
      when: provision_node_exporter.changed

    - name: Provision Docker garbage-collector
      docker_swarm_service:
        name: garbage-collector
        state: present
        mode: global
        image: clockworksoul/docker-gc-cron:latest
        mounts:
          - source: /var/run/docker.sock
            target: /var/run/docker.sock
            type: bind
            readonly: no
        restart_config:
          condition: on-failure
        env:
          - CRON=25 11 * * *
          # By default, docker will not remove an image if it is tagged in multiple repositories. If
          # you have a server running docker where this is the case, for example in CI environments
          # where dockers are being built, re-tagged, and pushed, you can set this flag to 1 to override.
          - FORCE_IMAGE_REMOVAL=1
          # By default, if an error is encountered when cleaning up a container, Docker will report the
          # error back and leave it on disk. This can sometimes lead to containers accumulating. If
          # you run into this issue, you can force the removal of the container by setting this flag.
          - FORCE_CONTAINER_REMOVAL=1
          # By default, docker-gc will not remove a container if it exited less than 1 hour ago.
          # Set the GRACE_PERIOD_SECONDS variable to override this default.
          - GRACE_PERIOD_SECONDS=3600
          # By default, docker-gc will proceed with deletion of containers and images. To test your
          # settings set the DRY_RUN variable to override this default
          - DRY_RUN=0
          # By default, this process will leave any dangling volumes untouched. To instruct the
          # process to automatically clean up any dangling volumes, simply set this value to 1.
          - CLEAN_UP_VOLUMES=0
          # If you don't like all your log output and cron times being in UTC, you can set the
          # TZ variable to override the default.
          #- TZ=America/Chicago
          - MINIMUM_IMAGES_TO_SAVE=2
          - CLEAN_UP_VOLUMES=1
        limits:
          memory: 128M
        reservations:
          memory: 64M
        update_config:
          parallelism: 2
          delay: 60s
          monitor: 30s
          order: start-first
          failure_action: rollback
        placement:
          constraints: 
            - node.platform.os == linux
      tags:
        - garbage-collector
      register: provision_garbage_collector

    - name: Pause until Service is up
      pause: 
        prompt: "Make sure gargabe-collector is up and running"
        seconds: 30
      tags:
        - garbage-collector
      when: provision_garbage_collector.changed

    - name: Get garbage-collector Service Info
      docker_swarm_info:
        services: yes
        services_filters:
          name: garbage-collector
        verbose_output: yes
      register: garbage_collector_service
      tags:
        - garbage-collector

    - debug:
        var: garbage_collector_service

    - debug:
        var: provision_garbage_collector

    - name: Provision Portainer
      docker_swarm_service:
        command: /portainer -H tcp://tasks.portainer-agent:9001 --tlsskipverify --logo https://www.micro-biolytics.com/wp-content/uploads/2019/03/cropped-mBio_logo.png --no-analytics
        name: portainer
        state: present
        mode: replicated
        replicas: 1
        image: portainer/portainer:1.23.0
        env:
          AGENT_CLUSTER_ADDR: "tasks.portainer-agent"
          CAP_HOST_MANAGEMENT: "1"
        networks:
          - name: "proxy"
            aliases:
              - "traefik"
            options:
              driver: "overlay"
          - name: "portainer"
            aliases:
              - "portainer"
            options:
              driver: "overlay"
        mounts:
          - source: portainer-data-cio
            target: /data
            type: volume
            readonly: no
            driver_config: 
              name: cio
        restart_config:
          condition: on-failure
        update_config:
          parallelism: 2
          delay: 60s
          monitor: 30s
          order: start-first
          failure_action: rollback
        placement:
          constraints: 
            - node.role == manager
        labels:
          traefik.enable: "true"
          traefik.tags: "proxy"
          traefik.docker.network: "proxy"
          traefik.port: "9000"
          traefik.frontend.rule: "Host:swarm.{{ base_domain }}"
          traefik.backend: "portainer-swarm"
          traefik.protocol: "http"
          traefik.frontend.headers.SSLRedirect: "true"
          traefik.frontend.entryPoints: "http,https"
          traefik.backend.loadbalancer.stickiness: "true"
      register: provision_portainer

    - name: Create A record at Cloudflare
      cloudflare_dns:
        zone: "{{ base_domain }}"
        record: "swarm"
        type: A
        value: "{{ ansible_default_ipv4.address }}"
        account_email: "{{ cf_api_email }}"
        account_api_token: "{{ cf_api_key }}"
        solo: yes
      register: record

    - name: "Wait for swarm.{{ base_domain }} to become ready"
      uri:
        url: "https://swarm.{{ base_domain }}"
        method: GET
        status_code: 200
      register: _result
      until: _result.status == 200
      retries: 30 # retry X times  
      delay: 5 # pause for X sec b/w each call

    - name: Provision Consul Leader
      docker_swarm_service:
        command: >
          /usr/local/bin/docker-entrypoint.sh agent -server -client=0.0.0.0 -bootstrap -ui
        name: consul-leader
        state: present
        mode: replicated
        replicas: 1
        image: consul:1.6.2
        env:
          CONSUL_BIND_INTERFACE: "eth0"
          CONSUL_LOCAL_CONFIG: '{"leave_on_terminate": true}'
        networks:
          - consul
          - proxy
        mounts:
          - source: consul-data-leader-cio
            target: /consul/data
            type: volume
            readonly: no
            driver_config: 
              name: cio
        restart_config:
          condition: on-failure
        placement:
          constraints:
            - node.role == manager
        labels:
          traefik.enable: "true"
          traefik.tags: "proxy"
          traefik.docker.network: "proxy"
          traefik.port: "8500"
          traefik.frontend.rule: "Host:consul.{{ base_domain }}"
          traefik.backend: "consul"
          traefik.protocol: "http"
          traefik.frontend.headers.SSLRedirect: "true"
          traefik.frontend.entryPoints: "http,https"
          traefik.backend.loadbalancer.stickiness: "true"
      register: provision_consul_leader

    - name: "Wait for consul.{{ base_domain }} to become ready"
      uri:
        url: "https://consul.{{ base_domain }}"
        method: GET
        http_agent: "if0"
        status_code: 200
        #user: if0
        #password: if0
      register: _result
      until: _result.status == 200
      retries: 30 # retry X times  
      delay: 5 # pause for X sec b/w each call 
    
    # - name: Provision Consul Replicas
    #   docker_swarm_service:
    #     command: >
    #      /usr/local/bin/docker-entrypoint.sh agent -server -client=0.0.0.0 -retry-join="consul-leader"
    #     name: consul-replica
    #     state: present
    #     mode: replicated
    #     replicas: 3
    #     image: consul:1.6.2
    #     env:
    #       CONSUL_BIND_INTERFACE: "eth0"
    #       CONSUL_LOCAL_CONFIG: '{"leave_on_terminate": true}'
    #     networks:
    #       - consul
    #       - proxy
    #     mounts:
    #       - source: '{{ "{{.Service.Name}}-{{.Task.Slot}}" }}'
    #         target: /consul/data
    #         type: volume
    #         readonly: no
    #         driver_config: 
    #           name: cio
    #           # options:
    #           #   volume: '{{ "{{.Service.Name}}-{{.Task.Slot}}" }}'
    #     restart_config:
    #       condition: on-failure
    #     update_config:
    #       parallelism: 1
    #       delay: 120s
    #       monitor: 120s
    #       order: start-first
    #       failure_action: rollback
    #     placement:
    #       constraints: 
    #         - node.role == manager
    #       preferences:
    #         - spread: node.id
    #   register: provision_consul_replica

    # - name: Pause until Service is up
    #   pause: 
    #     prompt: "Make sure consul-replica is up and running"
    #     seconds: 30
    #   when: provision_consul_replica.changed

    - name: Provision Prometheus
      docker_swarm_service:
        command: /bin/prometheus --config.file=/etc/prometheus/prometheus.yml --web.console.libraries=/etc/prometheus/console_libraries --web.console.templates=/etc/prometheus/consoles --storage.tsdb.path=/prometheus --storage.tsdb.retention.time=48h --web.enable-admin-api
        name: prometheus
        state: present
        mode: replicated
        replicas: 1
        image: prom/prometheus:latest
        networks:
          - name: "proxy"
            aliases:
              - "traefik"
            options:
              driver: "overlay"
          - name: "monitoring"
            aliases:
              - "monitoring"
            options:
              driver: "overlay"
        mounts:
          - source: prometheus-data
            target: /prometheus
            type: volume
            readonly: no
        restart_config:
          condition: on-failure
        update_config:
          parallelism: 2
          delay: 60s
          monitor: 30s
          order: start-first
          failure_action: rollback
        placement:
          constraints: 
            - node.hostname == satellite-1
        limits:
          memory: 2048M
        reservations:
          memory: 1024M
        labels:
          traefik.enable: "true"
          traefik.tags: "proxy"
          traefik.docker.network: "proxy"
          traefik.port: "9090"
          traefik.frontend.rule: "Host:prometheus.{{ base_domain }}"
          traefik.backend: "prometheus"
          traefik.protocol: "http"
          traefik.frontend.headers.SSLRedirect: "true"
          traefik.frontend.entryPoints: "http,https"
          traefik.backend.loadbalancer.stickiness: "true"
          prometheus.enable: "true"
          prometheus.port: "9090"
          prometheus.path: "/metrics"
        healthcheck:
          test: "/bin/wget -q -Y off http://localhost:9090/status -O /dev/null > /dev/null 2>&1"
          interval: 25s
          timeout: 3s
          start_period: 60s
      register: provision_prometheus

    - name: Create A record at Cloudflare
      cloudflare_dns:
        zone: "{{ base_domain }}"
        record: "prometheus"
        type: A
        value: "{{ ansible_default_ipv4.address }}"
        account_email: "{{ cf_api_email }}"
        account_api_token: "{{ cf_api_key }}"
        solo: yes
      register: record

    - name: "Wait for prometheus.{{ base_domain }} to become ready"
      uri:
        url: "https://prometheus.{{ base_domain }}"
        method: GET
        status_code: 200
      register: _result
      until: _result.status == 200
      retries: 30 # retry X times  
      delay: 5 # pause for X sec b/w each call