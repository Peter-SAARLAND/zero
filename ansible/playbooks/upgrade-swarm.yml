- hosts: all:!manager[0]
  become: true
  remote_user: "{{ deploy_user_name }}"
  serial: 1
  tasks:
    - name: Docker Info
      command: docker info
      register: docker_info
      changed_when: False

    - name: Check for upgradeable
      become: True
      apt:
        upgrade: Yes
      check_mode: True
      register: upgradeable

    - name: Show me
      debug:
        msg: "{{ upgradeable }}"

    - name: Check Storidge Installation Status
      shell: command -v cio >/dev/null 2>&1
      register: cio_installed
      ignore_errors: yes
      when: upgradeable.changed

    - name: Drain CIO Node
      shell: cioctl node cordon '{{ hostvars[inventory_hostname]['ansible_hostname'] }}'
      delegate_to: "{{groups['manager'][0]}}"
      register: drain_status
      when: upgradeable.changed and cio_installed.rc == 0

    - name: Drain Swarm Node
      shell: docker node update --availability drain '{{ hostvars[inventory_hostname]['ansible_hostname'] }}'
      delegate_to: "{{groups['manager'][0]}}"
      register: drain_status
      when: upgradeable.changed and cio_installed.rc != 0

    - name: Upgrade Swarm Node
      apt:
        update_cache: yes
        upgrade: dist
        autoremove: yes
      #when: "'drain_status' in hostvars[inventory_hostname] and hostvars[inventory_hostname]['drain_status'].rc == 0"
      when: drain_status is succeeded
      register: upgrade

    - name: Check if reboot is required
      stat:
        path: /var/run/reboot-required
      register: reboot_required

    - name: Reboot Prompt
      pause:
        prompt: "Press ENTER to reboot {{ inventory_hostname }} now, or Ctrl+C to abort."
      # We need to check for the existence of 'reboot_required_file' first because play_hosts also
      # include hosts that have failed. When a host has failed, it stops executing and thus doesn't
      # have 'reboot_required_file'. And if we try to access 'stat', boom! failure. We don't want that.
      when: "'reboot_required' in hostvars[inventory_hostname] and hostvars[inventory_hostname]['reboot_required'].stat.exists"

    - name: Rebooting Swarm Node
      shell: sleep 2 && shutdown -r now "Ansible updates triggered"
      async: 1
      poll: 0
      ignore_errors: true
      when: reboot_required.stat.exists

    - name: Waiting for Swarm Node to come back
      local_action: wait_for host={{ ansible_host }} state=started port=22 delay=30
      become: no
      when: reboot_required.stat.exists

    # - name: Reboot Swarm Node
    #   reboot:
    #     reboot_timeout: 300
    #     test_command: docker info
    #     msg: "Rebooting Node"
    #   when: upgrade.changed
      
    # - name: Wait for Swarm Node to become available again
    #   wait_for_connection:
    #       delay: 5
    #       timeout: 180
    #   when: upgrade.changed

    # - name: Wait for Swarm Manager
    #   wait_for:
    #     host: "{{ inventory_hostname }}"
    #     port: "2377"
    #     delay: 5
    #     state: started
    #     timeout: 1200

    - name: Activate CIO Node
      shell: cioctl node uncordon '{{ hostvars[inventory_hostname]['ansible_hostname'] }}'
      delegate_to: "{{groups['manager'][0]}}"
      when: drain_status.changed and cio_installed.rc == 0
      register: activate_status

    - name: Activate Swarm Node
      shell: docker node update --availability active '{{ hostvars[inventory_hostname]['ansible_hostname'] }}'
      delegate_to: "{{groups['manager'][0]}}"
      register: drain_status
      when: upgradeable.changed and cio_installed.rc != 0
    
    # - name: Reboot if kernel/libs was updated
    #   shell: sleep 10 && /sbin/shutdown -r now 'Rebooting box to update system libs/kernel as needed' 
    #   args:
    #       removes: /var/run/reboot-required
    #   async: 300
    #   poll: 0
    #   ignore_errors: true

- hosts: manager[0]
  become: true
  remote_user: "{{ deploy_user_name }}"
  serial: 1
  tasks:
    - name: Docker Info
      command: docker info
      register: docker_info
      changed_when: False

    - name: Check for upgradeable
      become: True
      apt:
        upgrade: Yes
      check_mode: True
      register: upgradeable

    - name: Show me
      debug:
        msg: "{{ upgradeable }}"

    - name: Check Storidge Installation Status
      shell: command -v cio >/dev/null 2>&1
      register: cio_installed
      ignore_errors: yes
      when: upgradeable.changed

    - name: Drain CIO Node
      shell: cioctl node cordon '{{ hostvars[inventory_hostname]['ansible_hostname'] }}'
      delegate_to: "{{groups['manager'][1]}}"
      register: drain_status
      when: upgradeable.changed and cio_installed.rc == 0

    - name: Drain Swarm Node
      shell: docker node update --availability drain '{{ hostvars[inventory_hostname]['ansible_hostname'] }}'
      delegate_to: "{{groups['manager'][1]}}"
      register: drain_status
      when: upgradeable.changed and cio_installed.rc != 0

    - name: Upgrade Swarm Node
      apt:
        update_cache: yes
        upgrade: dist
        autoremove: yes
      #when: "'drain_status' in hostvars[inventory_hostname] and hostvars[inventory_hostname]['drain_status'].rc == 0"
      when: drain_status is succeeded
      register: upgrade

    - name: Check if reboot is required
      stat:
        path: /var/run/reboot-required
      register: reboot_required

    - name: Reboot Prompt
      pause:
        prompt: "Press ENTER to reboot {{ inventory_hostname }} now, or Ctrl+C to abort."
      # We need to check for the existence of 'reboot_required_file' first because play_hosts also
      # include hosts that have failed. When a host has failed, it stops executing and thus doesn't
      # have 'reboot_required_file'. And if we try to access 'stat', boom! failure. We don't want that.
      when: "'reboot_required' in hostvars[inventory_hostname] and hostvars[inventory_hostname]['reboot_required'].stat.exists"

    - name: Rebooting Swarm Node
      shell: sleep 2 && shutdown -r now "Ansible updates triggered"
      async: 1
      poll: 0
      ignore_errors: true
      when: reboot_required.stat.exists

    - name: Waiting for Swarm Node to come back
      local_action: wait_for host={{ ansible_host }} state=started port=22 delay=30
      become: no
      when: reboot_required.stat.exists

    # - name: Reboot Swarm Node
    #   reboot:
    #     reboot_timeout: 300
    #     test_command: docker info
    #     msg: "Rebooting Node"
    #   when: upgrade.changed
      
    # - name: Wait for Swarm Node to become available again
    #   wait_for_connection:
    #       delay: 5
    #       timeout: 180
    #   when: upgrade.changed

    # - name: Wait for Swarm Manager
    #   wait_for:
    #     host: "{{ inventory_hostname }}"
    #     port: "2377"
    #     delay: 5
    #     state: started
    #     timeout: 1200

    - name: Activate CIO Node
      shell: cioctl node uncordon '{{ hostvars[inventory_hostname]['ansible_hostname'] }}'
      delegate_to: "{{groups['manager'][1]}}"
      when: drain_status.changed and cio_installed.rc == 0
      register: activate_status

    - name: Activate Swarm Node
      shell: docker node update --availability active '{{ hostvars[inventory_hostname]['ansible_hostname'] }}'
      delegate_to: "{{groups['manager'][1]}}"
      register: drain_status
      when: upgradeable.changed and cio_installed.rc != 0
    
    # - name: Reboot if kernel/libs was updated
    #   shell: sleep 10 && /sbin/shutdown -r now 'Rebooting box to update system libs/kernel as needed' 
    #   args:
    #       removes: /var/run/reboot-required
    #   async: 300
    #   poll: 0
    #   ignore_errors: true
